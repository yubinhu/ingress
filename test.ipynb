{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1376 chunks.\n",
      "Chunk(id=b45da1fe16507a1b9033dbad46463c6c05fa74e6f099e9d7a82993c99e378a96_0, doc_id=b45da1fe16507a1b9033dbad46463c6c05fa74e6f099e9d7a82993c99e378a96, text=Roman Abramovich has..., embedding=[0.019449638202786446, 0.007399196736514568, 0.0320294015109539, 0.043133825063705444, -0.0119040347635746])\n"
     ]
    }
   ],
   "source": [
    "from my_package.load import FootballNewsLoader, FinancialNewsLoader\n",
    "from my_package.document_chunker import DocumentChunker\n",
    "from my_package.embed import Embedder\n",
    "\n",
    "LIMIT = 100\n",
    "\n",
    "# Step 1: Load data\n",
    "loader = FootballNewsLoader(directory_path='data/football')\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Chunk documents\n",
    "chunker = DocumentChunker()\n",
    "all_chunks = []\n",
    "for document in documents[:LIMIT]:\n",
    "    chunks = chunker.chunk(document)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# Step 3: Embed chunks\n",
    "embedder = Embedder()\n",
    "embedded_chunks = embedder.embed(all_chunks)\n",
    "\n",
    "# Further processing or saving embedded_chunks\n",
    "print(f\"Processed {len(embedded_chunks)} chunks.\")\n",
    "print(all_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todos\n",
    "\n",
    "[P1]\n",
    "- [x] Process all files in football folder\n",
    "\n",
    "- [x] Implement the financial dataset loader\n",
    "\n",
    "- [x] Add a real embedder\n",
    "\n",
    "\n",
    "[P2] \n",
    "- [x] Data cleaning\n",
    "\n",
    "- [ ] Add publish time to the document\n",
    "    Plan: publish time is another critical feature that would be useful in training and testing purposes. We can add the publish time to the document data structure and drop any document that does not have a publish time. This would help further cleaning up the data. \n",
    "\n",
    "- [ ] Improve chuncker\n",
    "    Plan: chunking the document with overlapping windows would allow for better search results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1116 chunks.\n",
      "Chunk(id=4e41266ca1707a052245161948413f057982c0b2_0, doc_id=4e41266ca1707a052245161948413f057982c0b2, text=March 27(Reuters) - ..., embedding=[0.029279867187142372, -0.0169009268283844, 0.017760103568434715, 0.0007397688459604979, -0.022756900638341904])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load data\n",
    "loader = FinancialNewsLoader(directory_path='data/financial')\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Chunk documents\n",
    "chunker = DocumentChunker()\n",
    "all_chunks = []\n",
    "for document in documents[:LIMIT]:\n",
    "    chunks = chunker.chunk(document)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# Step 3: Embed chunks\n",
    "embedder = Embedder()\n",
    "embedded_chunks = embedder.embed(all_chunks)\n",
    "\n",
    "# Further processing or saving embedded_chunks\n",
    "print(f\"Processed {len(embedded_chunks)} chunks.\")\n",
    "print(all_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
